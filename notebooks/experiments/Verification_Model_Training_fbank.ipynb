{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Verification-Model-Training-fbank.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preliminaries"
      ],
      "metadata": {
        "id": "46DmsB1IXXfy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8xosre2tUD2V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9e24be6-bcc5-4be3-b9b5-d1f343b24ed1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    'outputs':['/gdrive/MyDrive/shared_space/commonvoice-farsi/fbanks-1.npy',\n",
        "               '/gdrive/MyDrive/shared_space/commonvoice-farsi/fbanks-2.npy',\n",
        "               '/gdrive/MyDrive/shared_space/commonvoice-farsi/fbanks-3.npy',\n",
        "               '/gdrive/MyDrive/shared_space/commonvoice-farsi/fbanks-4.npy',\n",
        "               '/gdrive/MyDrive/shared_space/commonvoice-farsi/fbanks-5.npy',\n",
        "               '/gdrive/MyDrive/shared_space/commonvoice-farsi/fbanks-6.npy'],\n",
        "    'positive_pairs':'/gdrive/MyDrive/shared_space/commonvoice-farsi/positive-pairs.pkl',\n",
        "    'negative_pairs':'/gdrive/MyDrive/shared_space/commonvoice-farsi/negative-pairs.pkl',\n",
        "    'path_to_part':'/gdrive/MyDrive/shared_space/commonvoice-farsi/path-to-part.pkl',\n",
        "    'path_to_index':'/gdrive/MyDrive/shared_space/commonvoice-farsi/path-to-index.pkl',\n",
        "    'models_path':'/gdrive/MyDrive/arman/verification-models/',\n",
        "    'num_epochs':200,\n",
        "    'learning_rate':1e-1,\n",
        "    'batch_size':32,\n",
        "    'train_percentage':90\n",
        "}"
      ],
      "metadata": {
        "id": "rCBkvsQ1ULbT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pickle as pkl\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "wSJYewZfaUEJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the Data"
      ],
      "metadata": {
        "id": "K3i9Kt9tXhuc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# outputs = [np.load(f'/gdrive/MyDrive/shared_space/commonvoice-farsi/fbanks-{i}.npy', allow_pickle=True) for i in range(1,7)]"
      ],
      "metadata": {
        "id": "b7Sd2Y5HXiv-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# outputs[0].shape"
      ],
      "metadata": {
        "id": "ettsBzcOaiRj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# outputs[3].shape"
      ],
      "metadata": {
        "id": "ccvuamPfam7h"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with open(config['positive_pairs'], 'rb') as file:\n",
        "#   pos_pairs = pkl.load(file)\n",
        "# with open(config['negative_pairs'], 'rb') as file:\n",
        "#   neg_pairs = pkl.load(file)\n",
        "# with open(config['path_to_part'], 'rb') as file:\n",
        "#   path_to_part = pkl.load(file)\n",
        "# with open(config['path_to_index'], 'rb') as file:\n",
        "#   path_to_index = pkl.load(file)"
      ],
      "metadata": {
        "id": "_H4OCsrraphC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pos_pairs[0]"
      ],
      "metadata": {
        "id": "0AZTRNxZjdpP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pair_to_array(pair):\n",
        "  first_array = outputs[int(path_to_part[pair[0]])-1][path_to_index[pair[0]]]\n",
        "  second_array = outputs[int(path_to_part[pair[1]])-1][path_to_index[pair[1]]]\n",
        "  return first_array, second_array"
      ],
      "metadata": {
        "id": "iNcXILiij6er"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pair_to_array(pos_pairs[0])[0].shape"
      ],
      "metadata": {
        "id": "mCwk02aGkoPN"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pair_to_array(pos_pairs[0])[1].shape"
      ],
      "metadata": {
        "id": "LtqAY7wHCDjj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pos_arrays = np.array([pair_to_array(pair) for pair in pos_pairs])\n",
        "# neg_arrays = np.array([pair_to_array(pair) for pair in neg_pairs])"
      ],
      "metadata": {
        "id": "Hxbdy26ylxgy"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pos_arrays.shape"
      ],
      "metadata": {
        "id": "H_YkS3BRl8as"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# neg_arrays.shape"
      ],
      "metadata": {
        "id": "sat0xxDJw_iw"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# max_frames = 0\n",
        "# all_lengths = []\n",
        "# for item in pos_arrays:\n",
        "#   all_lengths.append(item[0].shape[0])\n",
        "#   all_lengths.append(item[1].shape[0])\n",
        "#   curr_max = max(item[0].shape[0], item[1].shape[0])\n",
        "#   if curr_max > max_frames:\n",
        "#     max_frames = curr_max"
      ],
      "metadata": {
        "id": "jKEeTFAPCNTO"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# max_frames"
      ],
      "metadata": {
        "id": "WZ9BkkX4CvS1"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sum(all_lengths)/len(all_lengths)"
      ],
      "metadata": {
        "id": "sCfZA9YfCw7s"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pos_arrays[0][0].shape"
      ],
      "metadata": {
        "id": "FxoUo3nME44p"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pos_arrays[0][0][:200].shape"
      ],
      "metadata": {
        "id": "wisDt08VFdPc"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_length(arr, desired_length=500):\n",
        "  if arr.shape[0]<desired_length:\n",
        "    tmp = np.zeros((desired_length-arr.shape[0], 40))\n",
        "    output = np.concatenate((arr, tmp), axis=0)\n",
        "  elif arr.shape[0]>desired_length:\n",
        "    output = arr[:desired_length]\n",
        "  else:\n",
        "    output = arr\n",
        "  return output"
      ],
      "metadata": {
        "id": "N4MoHcCCDyhO"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# outputs = [np.load(f'/gdrive/MyDrive/shared_space/commonvoice-farsi/fbanks-{i}.npy', allow_pickle=True) for i in range(1,7)]\n",
        "# with open(config['positive_pairs'], 'rb') as file:\n",
        "#   pos_pairs = pkl.load(file)\n",
        "# with open(config['negative_pairs'], 'rb') as file:\n",
        "#   neg_pairs = pkl.load(file)\n",
        "# with open(config['path_to_part'], 'rb') as file:\n",
        "#   path_to_part = pkl.load(file)\n",
        "# with open(config['path_to_index'], 'rb') as file:\n",
        "#   path_to_index = pkl.load(file)\n",
        "# pos_arrays = np.array([pair_to_array(pair) for pair in pos_pairs])\n",
        "# neg_arrays = np.array([pair_to_array(pair) for pair in neg_pairs])\n",
        "\n",
        "# tmp_pos = pos_arrays.tolist()\n",
        "# tmp_pos = np.array([[fix_length(item[0]), fix_length(item[1])] for item in tmp_pos])\n",
        "# tmp_neg = neg_arrays.tolist()\n",
        "# tmp_neg = np.array([[fix_length(item[0]), fix_length(item[1])] for item in tmp_neg])\n",
        "# data = np.concatenate((tmp_pos, tmp_neg), axis=0)\n",
        "# labels = np.array([1 if i < pos_arrays.shape[0] else 0 for i in range(data.shape[0])])\n",
        "# np.save('/gdrive/MyDrive/shared_space/commonvoice-farsi/data.npy', data, allow_pickle=True)\n",
        "# np.save('/gdrive/MyDrive/shared_space/commonvoice-farsi/labels.npy', labels, allow_pickle=True)"
      ],
      "metadata": {
        "id": "_NpxOMSzGUJ_"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.load('/gdrive/MyDrive/shared_space/commonvoice-farsi/data.npy', allow_pickle=True)\n",
        "labels = np.load('/gdrive/MyDrive/shared_space/commonvoice-farsi/labels.npy', allow_pickle=True)"
      ],
      "metadata": {
        "id": "vLq1aM8UJn0w"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splitting the data into training, testing and validation (development) parts"
      ],
      "metadata": {
        "id": "t3F9JCxCxrzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f_train, f_rem, l_train, l_rem = train_test_split(data, labels, test_size=1-config['train_percentage']/100, random_state=50)\n",
        "f_test, f_dev, l_test, l_dev = train_test_split(f_rem, l_rem, test_size=0.5, random_state=50)"
      ],
      "metadata": {
        "id": "oUfQrItsxCm1"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'train features: {f_train.shape}, dev features: {f_dev.shape}, test features: {f_test.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usKOBO-MzRu4",
        "outputId": "ccfb6826-1d87-45f3-c2b3-cb2f39ad9d50"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train features: (19197, 2, 500, 40), dev features: (1067, 2, 500, 40), test features: (1067, 2, 500, 40)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'train labels: {len(l_train)}, dev labels: {len(l_dev)}, test labels: {len(l_test)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKkmIo7yzSgB",
        "outputId": "9d6e5068-6a52-45cd-dfa5-155c76c047d6"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train labels: 19197, dev labels: 1067, test labels: 1067\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Converting Everything to Tensors"
      ],
      "metadata": {
        "id": "jHlhl6l1zgAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "piMMrhmNzuvA",
        "outputId": "928dee4e-bc80-48bc-bbfb-9ea2a468a5fa"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, features, labels, device):\n",
        "        self.features = torch.from_numpy(features).to(device)\n",
        "        self.labels = torch.from_numpy(np.array(labels).reshape(-1, 1)).to(device)\n",
        "    def __len__(self):\n",
        "        return self.features.shape[0]\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx]"
      ],
      "metadata": {
        "id": "hZRWC6yozUUR"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = MyDataset(f_train, l_train, device)\n",
        "test_dataset = MyDataset(f_test, l_test, device)\n",
        "dev_dataset = MyDataset(f_dev, l_dev, device)"
      ],
      "metadata": {
        "id": "lEcQrcWWzmSb"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=True)\n",
        "dev_dataloader = DataLoader(dev_dataset, batch_size=config['batch_size'], shuffle=True)"
      ],
      "metadata": {
        "id": "GAViccUIzp3_"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0][0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwShv9Wv1ZZq",
        "outputId": "7b868756-e8a1-43cb-b65a-21d21348d2bd"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 500, 40])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network Architecture"
      ],
      "metadata": {
        "id": "j73QWJohztcy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class my_neural_net(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(my_neural_net, self).__init__() \n",
        "        self.lstm = nn.LSTM(40, 20, 1, batch_first=True)\n",
        "        self.flatten = torch.nn.Flatten()\n",
        "        self.seq_layer = torch.nn.Sequential( \n",
        "            nn.Linear(40, 20),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(20, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1,0)\n",
        "        first_seq = x[0].to(torch.float32)\n",
        "        second_seq = x[1].to(torch.float32)\n",
        "        first_lstm_out = self.lstm(first_seq)[1][0]\n",
        "        second_lstm_out = self.lstm(second_seq)[1][0]\n",
        "        # return first_lstm_out, second_lstm_out\n",
        "        concatenated = torch.cat((first_lstm_out, second_lstm_out), 2)\n",
        "        # return concatenated\n",
        "        concatenated = torch.squeeze(concatenated)\n",
        "        # return concatenated\n",
        "        # flattened = self.flatten(concatenated)\n",
        "        output = self.seq_layer(concatenated)\n",
        "        return output"
      ],
      "metadata": {
        "id": "tIsdM-zVzvdv"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_nn = my_neural_net()\n",
        "my_nn = my_nn.to(device)"
      ],
      "metadata": {
        "id": "WDSBIBX80A8x"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[:5][0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPi8JjoVHMDy",
        "outputId": "571fd5e1-2266-4def-9706-044257372ba0"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 2, 500, 40])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(my_nn(train_dataset[:5][0])).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-AibiYK0IT8",
        "outputId": "b33c9b1d-782c-4cd9-8fa2-c3dc8d77e9b9"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_nn(train_dataset[:3][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_61R5oXR0DAt",
        "outputId": "0ad2f166-8489-418a-e0e4-f64b07fb66f2"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.5834],\n",
              "        [0.5916],\n",
              "        [0.5666]], device='cuda:0', grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8ISj-HmUtgV",
        "outputId": "a0af1cfe-d48d-4557-e8d7-393929985123"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(19197,)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "h7W1K6G21n-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.BCELoss()\n",
        "optimizer = torch.optim.SGD(my_nn.parameters(), lr=config['learning_rate'])"
      ],
      "metadata": {
        "id": "Xsg4S54m1ohY"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def output_to_label(out):\n",
        "    dist_to_0 = abs(out)\n",
        "    dist_to_1 = abs(out-1)\n",
        "    if dist_to_0 <= dist_to_1:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1"
      ],
      "metadata": {
        "id": "RHmTYeGS1t77"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer, epoch_num):\n",
        "    num_points = len(dataloader.dataset)\n",
        "    for batch, (features, labels) in enumerate(dataloader):        \n",
        "        # Compute prediction and loss\n",
        "        labels = labels.to(torch.float32)\n",
        "        pred = model(features).to(torch.float32)\n",
        "        loss = loss_fn(pred, labels)\n",
        "        \n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad() # sets gradients of all model parameters to zero\n",
        "        loss.backward() # calculate the gradients again\n",
        "        optimizer.step() # w = w - learning_rate * grad(loss)_with_respect_to_w\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(features)\n",
        "            print(f\"\\r Epoch {epoch_num} - loss: {loss:>7f}  [{current:>5d}/{num_points:>5d}]\", end=\" \")\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn, epoch_num, name):\n",
        "    num_points = len(dataloader.dataset)\n",
        "    sum_test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch, (features, labels) in enumerate(dataloader):\n",
        "            labels = labels.to(torch.float32)\n",
        "            pred = model(features).to(torch.float32)\n",
        "            sum_test_loss += loss_fn(pred, labels).item() # add the current loss to the sum of the losses\n",
        "            # convert the outputs of the model on the current batch to a numpy array\n",
        "            pred_lst = list(pred.cpu().numpy().squeeze())\n",
        "            pred_lst = [output_to_label(item) for item in pred_lst]\n",
        "            # convert the original labels corresponding to the current batch to a numpy array\n",
        "            output_lst = list(labels.cpu().numpy().squeeze()) \n",
        "            # determine the points for which the model is correctly predicting the label (add a 1 for each)\n",
        "            match_lst = [1 if p==o else 0 for (p, o) in zip(pred_lst, output_lst)] \n",
        "            # count how many points are labeled correctly in this batch and add the number to the overall count of the correct labeled points\n",
        "            correct += sum(match_lst) \n",
        "            \n",
        "    sum_test_loss /= num_points\n",
        "    correct /= num_points\n",
        "    print(f\"\\r Epoch {epoch_num} - {name} Error: Accuracy: {(100*correct):>0.1f}%, Avg loss: {sum_test_loss:>8f}\", end=\" \")"
      ],
      "metadata": {
        "id": "xjVKl28NzruZ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch_num in range(1, config['num_epochs']+1):\n",
        "    train_loop(train_dataloader, my_nn, loss_fn, optimizer, epoch_num)\n",
        "    test_loop(dev_dataloader, my_nn, loss_fn, epoch_num, 'Development/Validation')"
      ],
      "metadata": {
        "id": "LZVHma6L18mC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a099915f-51ae-47f0-b0d4-dccd6e28532a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 200 - Development/Validation Error: Accuracy: 83.8%, Avg loss: 0.013110 "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loop(train_dataloader, my_nn, loss_fn, epoch_num, 'Training')"
      ],
      "metadata": {
        "id": "2DyTOVSV3nfN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9770d643-6aaf-4593-9f53-914da2942710"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r Epoch 200 - Training Error: Accuracy: 85.5%, Avg loss: 0.010987 "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loop(test_dataloader, my_nn, loss_fn, epoch_num, 'Test')"
      ],
      "metadata": {
        "id": "-K-upbB12BCM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5ea029f-41d2-4254-bdc0-61c2bd04dde3"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r Epoch 200 - Test Error: Accuracy: 84.3%, Avg loss: 0.011854 "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_nn"
      ],
      "metadata": {
        "id": "Kl6NNr-h3K8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(my_nn.state_dict(), config['models_path']+\"neural_net4.pth\")"
      ],
      "metadata": {
        "id": "YCXH5gB94orQ"
      },
      "execution_count": 42,
      "outputs": []
    }
  ]
}